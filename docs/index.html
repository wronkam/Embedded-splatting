<!DOCTYPE html>
<!-- saved from url=(0035)https://nvlabs.github.io/stylegan3/ -->
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <title>
   Embedded Splatting
  </title>
  <meta content="Embedded Splatting" property="og:title">
  <meta content="https://wronkam.github.io/Embedded-splatting" property="og:url">
  <style type="text/css">
   :root {
    --small-thumb-border-radius: 2px;
    --larger-thumb-border-radius: 3px;
}

html {
  font-size: 14px;
  line-height: 1.6;
  font-family: Inter, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
    margin: 0px;
    padding: 0px;
}

.base-grid,
.n-header,
.n-byline,
.n-title,
.n-article,
.n-footer {
    display: grid;
    justify-items: stretch;
    grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
    grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media(min-width: 1000px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media (min-width: 1180px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }
    .grid {
        grid-column-gap: 32px;
    }

}

.base-grid {
  grid-column: screen;
}

/* default grid column assignments */
.n-title > *  {
  grid-column: text;
}

.n-article > *  {
  grid-column: text;
}

.n-header {
    height: 0px;
}

.n-footer {
    height: 60px;
}

.n-title {
    padding: 4rem 0 1.5rem;
}

.l-page {
    grid-column: page;
}

.l-article {
    grid-column: text;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

.pixelated {
    image-rendering: pixelated;
}

strong {
    font-weight: 600;
}

/*------------------------------------------------------------------*/
/* title */
.n-title h1 {
    font-family: "Barlow",system-ui,Arial,sans-serif;
    color:#082333;
    grid-column: text;
    font-size: 40px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
    text-align: center;
}

@media (min-width: 768px) {
    .n-title h1 {
        font-size: 50px;
    }
}

/*------------------------------------------------------------------*/
/* article */
.n-article {
    color: rgb(33, 40, 53);
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    padding-top: 2rem;
}

.n-article h2 {
    contain: layout style;
    font-weight: 600;
    font-size: 24px;
    line-height: 1.25em;
    margin: 2rem 0 1.5rem 0;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding-bottom: 1rem;
}

@media (min-width: 768px) {
    .n-article {
        line-height: 1.7;
    }

    .n-article h2 {
        font-size: 36px;
    }
}

/*------------------------------------------------------------------*/
/* byline */

.n-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}

.n-byline .byline {
  grid-column: text;
}

.byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

@media (min-width: 768px) {
.grid {
    grid-column-gap: 16px;
}
}

.n-byline p {
  margin: 0;
}

.n-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
}
.n-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
}

/*------------------------------------------------------------------*/
/* figures */
.figure {
    margin-top: 1.5rem;
    margin-bottom: 1rem;
}

figcaption, .figcaption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
}

ul.authors {
    list-style-type: none;
    padding: 0;
    margin: 0;
    text-align: center;
}
ul.authors li {
    padding: 0 0.5rem;
    display: inline-block;
}

ul.authors sup {
    color: rgb(126,126,126);
}

ul.authors.affiliations  {
    margin-top: 0.5rem;
}

ul.authors.affiliations li {
    color: rgb(126,126,126);
}

/* Download section columns.  This switches between two layouts::after

- two columns on larger viewport sizes: side-by-side paper thumb and links
- single column: no thumb
 */
.download-section {
    display: grid;
    grid-template-areas: "links";
}
.download-section h4 {
    margin-left: 2.5rem;
    display: block;
}
.download-thumb {
    grid-area: thumb;
    display: none;
}
.download-links {
    grid-area: links;
}
img.dropshadow {
    box-shadow: 0 1px 10px rgba(0,0,0, 0.3);
}

@media(min-width: 1180px) {
    .download-section {
        display: grid;
        grid-template-areas: "thumb links";
    }
    .download-thumb {
        display: block;
    }
}

/* For BibTeX */
pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

/* video caption */

.video {
    margin-top: 1.5rem;
    margin-bottom: 1.5rem;
}

.videocaption {
    display: flex;
    font-size: 16px;
    line-height: 1.5em;
    margin-bottom: 1rem;
    justify-content: center;
}
   .disable-selection {
         user-select: none;
    -moz-user-select: none; /* Firefox */
     -ms-user-select: none; /* Internet Explorer */
  -khtml-user-select: none; /* KHTML browsers (e.g. Konqueror) */
 -webkit-user-select: none; /* Chrome, Safari, and Opera */
 -webkit-touch-callout: none; /* Disable Android and iOS callouts*/
}

.hidden {
    display: none;
}

h3.figtitle {
    margin-top: 0;
    margin-bottom: 0;
}

.fig-title-line {
    grid-template-columns: 2fr 0.75fr;
}

.fig-thumb-image-row {
    grid-template-columns: 1fr 1fr;
    grid-template-rows: 1fr;
}

.fig-thumb-image-row-item {
    width: 100%;
    min-height: auto;
    border-radius: var(--small-thumb-border-radius);
}

.fig-dataset-button {
    border-color: rgba(0,0,0,0);
    border-width: 1px;
    border-style: solid;
    cursor: pointer;
    opacity: 0.6;
}

.fig-dataset-button.active {
    border-color: rgba(0,0,0,0.7);
    border-width: 1px;
    border-style: solid;
    opacity: 1.0;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

.fig-3-image-row {
    margin-top: 1em;
    grid-template-columns: 1fr 1.3fr 1fr;
    grid-template-rows: 1fr;
}

.fig-3-image-item {
    justify-self: center;
    align-self: center;
    width: 100%;
    border-radius: var(--larger-thumb-border-radius);
}

/*---------------------------------------------------------------------*/
.fig-slider {
    grid-template-columns: auto 2fr;
    grid-template-rows: 1fr;
    margin-top: 1em;
    align-items: start;
    justify-content: center;
}

.fig-slider img.play_button {
    margin-right: 8px;
    cursor: pointer;
    justify-self: center;
}
.fig-slider svg {
    touch-action: none;
}

.fig-preload {
    display: none;
}
/*---------------------------------------------------------------------*/
  </style>
  <!-- inline stylesheet files into the above <style> element -->
  <link href="files/css" rel="stylesheet">
  <link as="image" href="files/paper-pdf-512.png" rel="preload">
  <link as="image" href="files/stylegan3-teaser-1920x1006.png" rel="preload">
 <link id="chromealerabat-link" rel="stylesheet" type="text/css" href="chrome-extension://pcajbjcmckcjacdpgmpadhmnpllndknb/content.css"></head>
 <body class="vsc-initialized">
  <div class="n-header">
  </div>
  <div class="n-title">
   <h1>
    Embedded-splatting
   </h1>
  </div>
  <div class="n-byline">
   <div class="byline">
    <ul class="authors">
     <li>
      Micha≈Ç Wronka
     </li>
   </div>
  </div>
  <div class="n-article">
   <div class="l-article">
     <img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/NN.png" alt="NN">
   </div>
   <h2 id="abstract">
       Introduction
   </h2>
          <p>This project aims to enrich <em>gaussian splatting</em> with an embedding for gaussians positions, which in theory, should allow them for faster, easier and more reliable learning of gaussins positions to better much reconstructed object.</p>
          <p>With the help of <a href="https://colab.research.google.com/github/camenduru/gaussian-splatting-colab/blob/main/gaussian_splatting_colab.ipynb">this notebook</a>, you can train <em>gaussian splatting</em> with or without embeddings, without writing any code or downloading anything, with all options choosable with simple UI with buttons and sliders. Additionally, this notebook contains the same explenation of how the model works as this page</p>
          <h3 id="contents-">Contents:</h3>
          <ul>
              <li><strong>Explanation</strong>: Explanation of the project motivation and solution.</li>
              <li><strong>Ablation</strong>: We present results of different model setups on synthetic NeRF drums dataset</li>
              <li><strong>Conclusion</strong>: Conclusion (as one could suspect)</li>
          </ul>
   <h2 id="Explanation">
    Explanation
   </h2>
    <h3 id="what-are-we-doing-exactly-">What are we doing exactly?</h3>
    <h4 id="gaussian-splatting">Gaussian splatting</h4>
    <p>Gaussian splatting can create 3D reconstructions of static scenes from a multi-camera capture, it means you can find a static object like a statue, walk around it and make a lot of images of it from different angles, give them to gaussian splatting and obtain rotatable, digital 3D object.</p>
    <h5 id="How-it-works">How it works</h5>
    <p>During training 2 main things are optimized. First the position of splats, as seen below</p>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/points.png" alt="points"></p>
    <p><small><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/gaussian/points.png"> Image</a></small></p>
    <p>The other thing are attributes of splats: color, opacity, size and how those are stretched</p>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/ellipsoids.png" alt="elipsoids"></p>
    <p><small><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/gaussian/ellipsoids.png"> Image</a></small></p>
    <p>Finally through rasterization, a rendering method, we can obtain a final view for a given camera position</p>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/bicycle.png" alt="bicycle"></p>
    <p><small><a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/gaussian/bicycle.png"> Image</a></small></p>
    <p>During training model adds new gaussians were needed and removes obsolete ones, for more details check <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">the paper at the authors website</a>.</p>
    <p>The ONLY mechanism through which the model knows if it does the good job is by comparing the created image with the original one.</p>
    <h5 id="an-issue">An issue</h5>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/half_ring.png" alt="ring"></p>
    <p>If we tried to reconstruct a ring (green), but had only gaussians on the left half of it. It would take a lot of iterations for gaussians to move/replicate and reach the right side of the ring.</p>
    <p>Could gaussians not move through the empty space, sadly NO, at each iterations the representation of the object (of any represented part of the object) must be valid, because it is compared with the original image.</p>
    <p>Can they jump across the empty space, also no. They always move only small distances, otherwise training would be too chaotic.</p>
    <p>An alternative we introduce is to project the position parameter through some neural network and use the way it bends the output space to make such moves easier.</p>
    <p>A complicated sentence so <strong>lets simplify it</strong>.</p>
    <p>We keep position parameter for each gauss. Originally those are  3 values (XYZ), corresponding directly to gausiann&#39;s position.
    Instead, we will use N (number can be adjusted) values that will be learnable and passed through a network to obtain gausiann&#39;s position,</p>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/NN.png" alt="NN"></p>
    <h4 id="-why-do-we-do-it-"><strong>Why</strong> do we do it?</h4>
    <p>Skipping all the math, each layer of neural network can be explained as set of operations on euclidian space. Imagine an object in space (cube maybe), when passed through a layer of network it will be, rotated, stretched, rotated again and translated (math word for moved).
    <img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/NN_steps.png" alt="NN_steps"></p>
    <p>Now the hard part. After each layer there is an activation function, which add some non-linear operation, like moving all <em>x</em> values below $0$ to exactly to $0$ (ReLU) or different scale below $0$ (LReLU) or do something more drastic (tanh, sigmoid, etc). Those must be added for math reasons and greatly expands the range of objects that the input, i.e cube, could be transformed into.</p>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/activation.png" alt="act"></p>
    <p><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary-width_case">Math wise</a>, 2 layers with activation can transform any object into any other object (when the dimension of those transformation, that is layer, is not measly 3D, but approaches infinite dimensions). So normally whe use more layers to compensate.</p>
    <h4 id="this-was-not-fun-i-asked-why-">This was not fun, I asked <strong>why</strong>?</h4>
    <p>If we pass our positions through network it could bend space in a way where moving on the surface of the reconstructed object is much simpler.
    Bellow there are circles again and the lines show how the input space (of positional parameters values) was bent without and with network. <strong>Red lines should be all about the same distance in parameter space, around 1.</strong> Using network allows gaussians to jump across empty space, because those jumps are now short, also it makes it harder to move in the wrong direction (look at the line on the right of both circles, when network is used gaussians would need to move much further to drift away.)</p>
    <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/circles.png" alt="circle"></p>
    <p>#%% md</p>
    <h3 id="there-is-more-">There is more.</h3>
    <p>Lets look again:
    <img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/NN.png" alt="NN"></p>
    <p>So about this green part. <a href="https://arxiv.org/abs/2006.10739">Reaserch</a> suggest that before passing some dense, low dimmensional (as in out case) input to the network it will be helpfull to use some specific embedding. Thats is, for example instead of $[x]$ pass $[x,\sin(x),\cos(x)]$ to our network.</p>
    <p>We have a few options:</p>
    <ul>
    <li><strong>none</strong>: no network, no embedding</li>
    <li><strong>net</strong>: just network, no embedding</li>
    <li><strong>fft</strong>: we calculate the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">fast fourier transform</a> of the positional parameter. Does not work well, but it is here.</li>
    <li><strong>positional</strong> and <strong>gff</strong>: <a href="https://arxiv.org/abs/1706.03762">Positional embedding</a> like in transformer models (yes, like Chat-GPT) and its cousin <a href="https://arxiv.org/abs/2006.10739">Gaussian fourier feature</a>. Both are some form of $[\sin(x),\cos(x)]$ embedding for each parameter variable, both can have <strong>embedding size</strong> set to change how long the embedding is (the $N$).
    Positional has set values of [a<sub>1</sub>,a<sub>2</sub>,...]$ variables as progressively smaller values $[1,1/2,1/4,...]$, and GFF uses random values from normal (gaussian) distribution as its variables. Both can be <strong>learnable</strong>, thst is, model will fine tune the values of $[a_1,a_2,...]$ variables. Those 2 show the best (similar) results.</li>
    </ul>
    <p><img width="45%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/main/images/embed.png" alt="embde"></p>
    <h3 id="last-one-">Last one, I promise.</h3>
    <p>You might want to ask if we need to change the training process so the model learns how to use the network. Here is the fun part, we do not. The network is optimized just like position parameters, only by comparing reconstruction and the original image.</p>

   <h2 id="citation">
    Ablation
   </h2>
      <p>Every test is done on drums images from nerf-synthetic dataset</p>
      <p><img width="100%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/drums.png" alt="drums"></p>
      <h3>
          Comparison of embeddings and residual connections
      </h3>
      <p>We tried to use residual connections to the model in a few variants, each having separate row in the table.</p>
      <p>The ideas behind those are respectably: just network, the baseline with no residual connection; output is confined to [-1,1] with hyperbolic tangent, and than stretched and translated with learnable S, T parameters; similar, but residual is added to TanH squeeze; solution 2 with added residual.</p>
      <p>Plus next to embedding signifies if its coefficients are optimized in training; if embedding is learnable.</p>
      <p>Positional parameter size is 3 for fair comparison with baseline.</p>
      <style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-08qx{background-color:#34ff34;border-color:inherit;color:#000000;text-align:center;vertical-align:top}
.tg .tg-461s{background-color:#96fffb;border-color:inherit;color:#000000;text-align:center;vertical-align:top}
.tg .tg-sp2s{background-color:#ffce93;border-color:inherit;color:#000000;font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-pb73{background-color:#9aff99;border-color:inherit;color:#000000;text-align:center;vertical-align:top}
.tg .tg-37xo{background-color:#ffffc7;border-color:inherit;color:#000000;font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-ml2k{border-color:inherit;color:#000000;text-align:center;vertical-align:top}
.tg .tg-8tj9{background-color:#ffffc7;border-color:inherit;color:#000000;text-align:center;vertical-align:top}
.tg .tg-jce5{background-color:#ffce93;border-color:inherit;color:#000000;text-align:center;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-ml2k"> </th>
    <th class="tg-sp2s"><span style="font-style:normal;text-decoration:none">Base</span></th>
    <th class="tg-37xo"><span style="font-style:normal;text-decoration:none">NNet</span></th>
    <th class="tg-37xo"><span style="font-style:normal;text-decoration:none">FFT</span></th>
    <th class="tg-37xo"><span style="font-style:normal;text-decoration:none">PE</span></th>
    <th class="tg-37xo"><span style="font-style:normal;text-decoration:none">PE+</span></th>
    <th class="tg-37xo"><span style="font-style:normal;text-decoration:none">GFF</span></th>
    <th class="tg-37xo"><span style="font-style:normal;text-decoration:none">GFF+</span></th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-37xo">Net(x)</td>
    <td class="tg-08qx"><span style="font-weight:400;font-style:normal;text-decoration:none">32.8</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">30.9</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">14.7</span></td>
    <td class="tg-461s"><span style="font-weight:400;font-style:normal;text-decoration:none">30.5</span></td>
    <td class="tg-pb73"><span style="font-weight:400;font-style:normal;text-decoration:none">30.7</span></td>
    <td class="tg-461s"><span style="font-weight:400;font-style:normal;text-decoration:none">29.3</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">27.8</span></td>
  </tr>
  <tr>
    <td class="tg-37xo"><span style="font-style:normal;text-decoration:none">Tanh(Net(x))*S+T</span></td>
    <td class="tg-jce5"> </td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">21.6</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">11.8</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">26.9</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">26.4</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">19.7</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">22.3</span></td>
  </tr>
  <tr>
    <td class="tg-37xo"><span style="font-style:normal;text-decoration:none">Tanh(Net(x)+x)*S+T</span></td>
    <td class="tg-jce5"> </td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">30.3</span></td>
    <td class="tg-461s"><span style="font-weight:400;font-style:normal;text-decoration:none">29.0</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">28.6</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">29.5</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">28.8</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">28.5</span></td>
  </tr>
  <tr>
    <td class="tg-37xo"><span style="font-style:normal;text-decoration:none">x+Tanh(Net(x))*S+T</span></td>
    <td class="tg-jce5"> </td>
    <td class="tg-461s"><span style="font-weight:400;font-style:normal;text-decoration:none">31.0</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">11.8</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">29.4</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">29.6</span></td>
    <td class="tg-8tj9"><span style="font-weight:400;font-style:normal;text-decoration:none">28.1</span></td>
    <td class="tg-461s"><span style="font-weight:400;font-style:normal;text-decoration:none">28.7</span></td>
  </tr>
</tbody></table>
      <h3>
          Comparison of different sizes of the position parameter
      </h3>
      <p>We are no longer confined to only 3 values, so lets test more. On the following images, red plot is always actual positions of splats and the green plot is projection of a hyper-dimensional cube through the embedding network to see the results of the space bending. Furthermore, image above red plot is original image and above green the reconstruction</p>
      <p>Every model is trained for 60k iterations.</p>
      <h3>
          Larger parameter size makes space bend better mach an object
      </h3>
      <p>From here we will always use learnable positional encoding with base output as it was the best in ablation.</p>
      <img width="150%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/num_dim.png" alt="dim_num" class="center">
      <h3>
          Parameter size = 2, step 1k, 10k, 30k, 60k
      </h3>
      <img width="150%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/dim_2.png" alt="dim_2" class="center">
      <h3>
          Parameter size = 3, step 1k, 10k, 30k, 60k
      </h3>
      <img width="150%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/dim_3.png" alt="dim_3" class="center">
      <h3>
          Parameter size = 4, step 1k, 10k, 30k, 60k
      </h3>
      <img width="150%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/dim_4.png" alt="dim_4" class="center">
      <h3>
          Parameter size = 8, step 1k, 10k, 30k, 60k
      </h3>
      <img width="150%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/dim_8.png" alt="dim_8" class="center">
      <h3>
          Baseline, step 1k, 10k, 30k, 60k
      </h3>
      <p>Green is the same as red here</p>
      <img width="150%" src="https://raw.githubusercontent.com/wronkam/Embedded-splatting/site/images/dim_base.png" alt="dim_base" class="center">
      <h2>Conclusion</h2>
      <p>Method has potential as it surely bends space and some runs suggested that it can very well recover from poor initialization and could potentially in future be used together with hypernetworks to make generative gaussian models.</p>
      <p>But for now it needs further improvement and testing until it matches the baseline or its improvements.</p>
  </div>
  <div class="n-footer">
  </div>
 
</body></html>